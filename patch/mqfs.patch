diff --git a/fs/fs-writeback.c b/fs/fs-writeback.c
index 471d8639..e1995f13 100644
--- a/fs/fs-writeback.c
+++ b/fs/fs-writeback.c
@@ -557,6 +557,7 @@ void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
 	if (unlikely(wb_dying(wbc->wb)))
 		inode_switch_wbs(inode, wbc->wb_id);
 }
+EXPORT_SYMBOL(wbc_attach_and_unlock_inode); // lxj add
 
 /**
  * wbc_detach_inode - disassociate wbc from inode and perform foreign detection
@@ -676,6 +677,7 @@ void wbc_detach_inode(struct writeback_control *wbc)
 	wb_put(wbc->wb);
 	wbc->wb = NULL;
 }
+EXPORT_SYMBOL(wbc_detach_inode); // lxj add 
 
 /**
  * wbc_account_io - account IO issued during writeback
@@ -2471,6 +2473,7 @@ int sync_inode(struct inode *inode, struct writeback_control *wbc)
 }
 EXPORT_SYMBOL(sync_inode);
 
+
 /**
  * sync_inode_metadata - write an inode to disk
  * @inode: the inode to sync
@@ -2490,3 +2493,152 @@ int sync_inode_metadata(struct inode *inode, int wait)
 	return sync_inode(inode, &wbc);
 }
 EXPORT_SYMBOL(sync_inode_metadata);
+
+// begin lxj add
+// see __writeback_single_inode()
+static int
+__writeback_single_inode_nowait_data(struct inode *inode, struct writeback_control *wbc)
+{
+	struct address_space *mapping = inode->i_mapping;
+	long nr_to_write = wbc->nr_to_write;
+	unsigned dirty;
+	int ret;
+
+	WARN_ON(!(inode->i_state & I_SYNC));
+
+	trace_writeback_single_inode_start(inode, wbc, nr_to_write);
+
+	ret = do_writepages(mapping, wbc);
+
+	/*
+	 * Make sure to wait on the data before writing out the metadata.
+	 * This is important for filesystems that modify metadata on data
+	 * I/O completion. We don't do it for sync(2) writeback because it has a
+	 * separate, external IO completion path and ->sync_fs for guaranteeing
+	 * inode metadata is written back correctly.
+	 */
+	// lxj add , we wait the completion of data blocks in __generic_file_atomic_write_wait()
+	// if (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync) {
+	// 	int err = filemap_fdatawait(mapping);
+	// 	if (ret == 0)
+	// 		ret = err;
+	// }
+
+	/*
+	 * Some filesystems may redirty the inode during the writeback
+	 * due to delalloc, clear dirty metadata flags right before
+	 * write_inode()
+	 */
+	spin_lock(&inode->i_lock);
+
+	dirty = inode->i_state & I_DIRTY;
+	if (inode->i_state & I_DIRTY_TIME) {
+		if ((dirty & I_DIRTY_INODE) ||
+		    wbc->sync_mode == WB_SYNC_ALL ||
+		    unlikely(inode->i_state & I_DIRTY_TIME_EXPIRED) ||
+		    unlikely(time_after(jiffies,
+					(inode->dirtied_time_when +
+					 dirtytime_expire_interval * HZ)))) {
+			dirty |= I_DIRTY_TIME | I_DIRTY_TIME_EXPIRED;
+			trace_writeback_lazytime(inode);
+		}
+	} else
+		inode->i_state &= ~I_DIRTY_TIME_EXPIRED;
+	inode->i_state &= ~dirty;
+
+	/*
+	 * Paired with smp_mb() in __mark_inode_dirty().  This allows
+	 * __mark_inode_dirty() to test i_state without grabbing i_lock -
+	 * either they see the I_DIRTY bits cleared or we see the dirtied
+	 * inode.
+	 *
+	 * I_DIRTY_PAGES is always cleared together above even if @mapping
+	 * still has dirty pages.  The flag is reinstated after smp_mb() if
+	 * necessary.  This guarantees that either __mark_inode_dirty()
+	 * sees clear I_DIRTY_PAGES or we see PAGECACHE_TAG_DIRTY.
+	 */
+	smp_mb();
+
+	if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))
+		inode->i_state |= I_DIRTY_PAGES;
+
+	spin_unlock(&inode->i_lock);
+
+	if (dirty & I_DIRTY_TIME)
+		mark_inode_dirty_sync(inode);
+	/* Don't write the inode if only I_DIRTY_PAGES was set */
+	if (dirty & ~I_DIRTY_PAGES) {
+		int err = write_inode(inode, wbc);
+		if (ret == 0)
+			ret = err;
+	}
+	trace_writeback_single_inode(inode, wbc, nr_to_write);
+	return ret;
+}
+
+// see writeback_single_inode()
+static int writeback_single_inode_nowait_data(struct inode *inode,
+				  struct writeback_control *wbc)
+{
+	struct bdi_writeback *wb;
+	int ret = 0;
+
+	spin_lock(&inode->i_lock);
+	if (!atomic_read(&inode->i_count))
+		WARN_ON(!(inode->i_state & (I_WILL_FREE|I_FREEING)));
+	else
+		WARN_ON(inode->i_state & I_WILL_FREE);
+
+	if (inode->i_state & I_SYNC) {
+		if (wbc->sync_mode != WB_SYNC_ALL)
+			goto out;
+		/*
+		 * It's a data-integrity sync. We must wait. Since callers hold
+		 * inode reference or inode has I_WILL_FREE set, it cannot go
+		 * away under us.
+		 */
+		__inode_wait_for_writeback(inode);
+	}
+	WARN_ON(inode->i_state & I_SYNC);
+	/*
+	 * Skip inode if it is clean and we have no outstanding writeback in
+	 * WB_SYNC_ALL mode. We don't want to mess with writeback lists in this
+	 * function since flusher thread may be doing for example sync in
+	 * parallel and if we move the inode, it could get skipped. So here we
+	 * make sure inode is on some writeback list and leave it there unless
+	 * we have completely cleaned the inode.
+	 */
+	if (!(inode->i_state & I_DIRTY_ALL) &&
+	    (wbc->sync_mode != WB_SYNC_ALL ||
+	     !mapping_tagged(inode->i_mapping, PAGECACHE_TAG_WRITEBACK)))
+		goto out;
+	inode->i_state |= I_SYNC;
+	wbc_attach_and_unlock_inode(wbc, inode);
+
+	ret = __writeback_single_inode_nowait_data(inode, wbc);
+
+	wbc_detach_inode(wbc);
+
+	wb = inode_to_wb_and_lock_list(inode);
+	spin_lock(&inode->i_lock);
+	/*
+	 * If inode is clean, remove it from writeback lists. Otherwise don't
+	 * touch it. See comment above for explanation.
+	 */
+	if (!(inode->i_state & I_DIRTY_ALL))
+		inode_io_list_del_locked(inode, wb);
+	spin_unlock(&wb->list_lock);
+	inode_sync_complete(inode);
+out:
+	spin_unlock(&inode->i_lock);
+	return ret;
+}
+
+// see sync_inode()
+int sync_inode_nowait_data(struct inode *inode, struct writeback_control *wbc)
+{
+	return writeback_single_inode_nowait_data(inode, wbc);
+}
+EXPORT_SYMBOL(sync_inode_nowait_data);
+
+// end lxj add
\ No newline at end of file
diff --git a/include/linux/fs.h b/include/linux/fs.h
index cf23c128..9028049f 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -1273,6 +1273,7 @@ extern void fasync_free(struct fasync_struct *);
 /* can be called from interrupts */
 extern void kill_fasync(struct fasync_struct **, int, int);
 
+extern int setfl(int fd, struct file * filp, unsigned long arg);
 extern void __f_setown(struct file *filp, struct pid *, enum pid_type, int force);
 extern int f_setown(struct file *filp, unsigned long arg, int force);
 extern void f_delown(struct file *filp);
@@ -1321,6 +1322,7 @@ extern int send_sigurg(struct fown_struct *fown);
 #define SB_I_NOEXEC	0x00000002	/* Ignore executables on this fs */
 #define SB_I_NODEV	0x00000004	/* Ignore devices on this fs */
 #define SB_I_MULTIROOT	0x00000008	/* Multiple roots to the dentry tree */
+#define SB_I_NOSUID	0x00000010	/* Ignore suid on this fs */
 
 /* sb->s_iflags to limit user namespace mounts */
 #define SB_I_USERNS_VISIBLE		0x00000010 /* fstype already mounted */
@@ -1736,6 +1738,7 @@ struct file_operations {
 	ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int);
 	unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
 	int (*check_flags)(int);
+	int (*setfl)(struct file *, unsigned long);
 	int (*flock) (struct file *, int, struct file_lock *);
 	ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int);
 	ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int);
@@ -1806,6 +1809,12 @@ ssize_t rw_copy_check_uvector(int type, const struct iovec __user * uvector,
 			      struct iovec *fast_pointer,
 			      struct iovec **ret_pointer);
 
+typedef ssize_t (*vfs_readf_t)(struct file *, char __user *, size_t, loff_t *);
+typedef ssize_t (*vfs_writef_t)(struct file *, const char __user *, size_t,
+				loff_t *);
+vfs_readf_t vfs_readf(struct file *file);
+vfs_writef_t vfs_writef(struct file *file);
+
 extern ssize_t __vfs_read(struct file *, char __user *, size_t, loff_t *);
 extern ssize_t vfs_read(struct file *, char __user *, size_t, loff_t *);
 extern ssize_t vfs_write(struct file *, const char __user *, size_t, loff_t *);
@@ -1858,6 +1867,10 @@ struct super_operations {
 				  struct shrink_control *);
 	long (*free_cached_objects)(struct super_block *,
 				    struct shrink_control *);
+#if defined(CONFIG_BLK_DEV_LOOP) ||  defined(CONFIG_BLK_DEV_LOOP_MODULE)
+	/* and aufs */
+	struct file *(*real_loop)(struct file *);
+#endif
 };
 
 /*
@@ -2093,6 +2106,9 @@ static inline void file_accessed(struct file *file)
 int sync_inode(struct inode *inode, struct writeback_control *wbc);
 int sync_inode_metadata(struct inode *inode, int wait);
 
+// lxj add
+int sync_inode_nowait_data(struct inode *inode, struct writeback_control *wbc);
+
 struct file_system_type {
 	const char *name;
 	int fs_flags;
@@ -2226,6 +2242,7 @@ extern int current_umask(void);
 extern void ihold(struct inode * inode);
 extern void iput(struct inode *);
 extern int generic_update_time(struct inode *, struct timespec64 *, int);
+extern int update_time(struct inode *, struct timespec64 *, int);
 
 /* /sys/fs */
 extern struct kobject *fs_kobj;
@@ -2513,6 +2530,7 @@ static inline bool sb_is_blkdev_sb(struct super_block *sb)
 	return false;
 }
 #endif
+extern int __sync_filesystem(struct super_block *, int);
 extern int sync_filesystem(struct super_block *);
 extern const struct file_operations def_blk_fops;
 extern const struct file_operations def_chr_fops;
@@ -2583,7 +2601,7 @@ static inline void unregister_chrdev(unsigned int major, const char *name)
 #define BLKDEV_MAJOR_MAX	512
 extern const char *__bdevname(dev_t, char *buffer);
 extern const char *bdevname(struct block_device *bdev, char *buffer);
-extern struct block_device *lookup_bdev(const char *);
+extern struct block_device *lookup_bdev(const char *, int mask);
 extern void blkdev_show(struct seq_file *,off_t);
 
 #else
@@ -2622,6 +2640,9 @@ extern int filemap_flush(struct address_space *);
 extern int filemap_fdatawait_keep_errors(struct address_space *mapping);
 extern int filemap_fdatawait_range(struct address_space *, loff_t lstart,
 				   loff_t lend);
+// lxj add
+extern int filemap_fdatawrite_tag_range(struct address_space *mapping, loff_t start, loff_t end, int tag);
+extern int filemap_fstreamwrite_tag_range(struct address_space *mapping, loff_t start, loff_t end, int tag, uint sid);
 
 static inline int filemap_fdatawait(struct address_space *mapping)
 {
@@ -3432,6 +3453,7 @@ static inline bool dir_relax_shared(struct inode *inode)
 }
 
 extern bool path_noexec(const struct path *path);
+extern bool path_nosuid(const struct path *path);
 extern void inode_nohighmem(struct inode *inode);
 
 #endif /* _LINUX_FS_H */
diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index fdfd04e3..fa358b66 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -68,6 +68,9 @@ struct writeback_control {
 	unsigned for_reclaim:1;		/* Invoked from the page allocator */
 	unsigned range_cyclic:1;	/* range_start is cyclic */
 	unsigned for_sync:1;		/* sync(2) WB_SYNC_ALL writeback */
+
+	// lxj add 
+	u32 stream; // stream ID 
 #ifdef CONFIG_CGROUP_WRITEBACK
 	struct bdi_writeback *wb;	/* wb this writeback is issued under */
 	struct inode *inode;		/* inode being written out */
diff --git a/mm/filemap.c b/mm/filemap.c
index 52517f28..122a686f 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -9,34 +9,34 @@
  * most "normal" filesystems (but you don't /have/ to use this:
  * the NFS filesystem used to do this differently, for example)
  */
-#include <linux/export.h>
+#include "internal.h"
+#include <linux/backing-dev.h>
+#include <linux/blkdev.h>
+#include <linux/capability.h>
+#include <linux/cleancache.h>
 #include <linux/compiler.h>
+#include <linux/cpuset.h>
 #include <linux/dax.h>
+#include <linux/export.h>
+#include <linux/file.h>
 #include <linux/fs.h>
-#include <linux/sched/signal.h>
-#include <linux/uaccess.h>
-#include <linux/capability.h>
-#include <linux/kernel_stat.h>
 #include <linux/gfp.h>
+#include <linux/hash.h>
+#include <linux/hugetlb.h>
+#include <linux/kernel_stat.h>
+#include <linux/memcontrol.h>
 #include <linux/mm.h>
-#include <linux/swap.h>
 #include <linux/mman.h>
 #include <linux/pagemap.h>
-#include <linux/file.h>
-#include <linux/uio.h>
-#include <linux/hash.h>
-#include <linux/writeback.h>
-#include <linux/backing-dev.h>
 #include <linux/pagevec.h>
-#include <linux/blkdev.h>
+#include <linux/rmap.h>
+#include <linux/sched/signal.h>
 #include <linux/security.h>
-#include <linux/cpuset.h>
-#include <linux/hugetlb.h>
-#include <linux/memcontrol.h>
-#include <linux/cleancache.h>
 #include <linux/shmem_fs.h>
-#include <linux/rmap.h>
-#include "internal.h"
+#include <linux/swap.h>
+#include <linux/uaccess.h>
+#include <linux/uio.h>
+#include <linux/writeback.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/filemap.h>
@@ -118,8 +118,8 @@ static int page_cache_tree_insert(struct address_space *mapping,
 	void **slot;
 	int error;
 
-	error = __radix_tree_create(&mapping->i_pages, page->index, 0,
-				    &node, &slot);
+	error = __radix_tree_create(&mapping->i_pages, page->index, 0, &node,
+				    &slot);
 	if (error)
 		return error;
 	if (*slot) {
@@ -156,14 +156,14 @@ static void page_cache_tree_delete(struct address_space *mapping,
 		struct radix_tree_node *node;
 		void **slot;
 
-		__radix_tree_lookup(&mapping->i_pages, page->index + i,
-				    &node, &slot);
+		__radix_tree_lookup(&mapping->i_pages, page->index + i, &node,
+				    &slot);
 
 		VM_BUG_ON_PAGE(!node && nr != 1, page);
 
 		radix_tree_clear_tags(&mapping->i_pages, node, slot);
 		__radix_tree_replace(&mapping->i_pages, node, slot, shadow,
-				workingset_lookup_update(mapping));
+				     workingset_lookup_update(mapping));
 	}
 
 	page->mapping = NULL;
@@ -172,11 +172,11 @@ static void page_cache_tree_delete(struct address_space *mapping,
 	if (shadow) {
 		mapping->nrexceptional += nr;
 		/*
-		 * Make sure the nrexceptional update is committed before
-		 * the nrpages update so that final truncate racing
-		 * with reclaim does not see both counters 0 at the
-		 * same time and miss a shadow entry.
-		 */
+     * Make sure the nrexceptional update is committed before
+     * the nrpages update so that final truncate racing
+     * with reclaim does not see both counters 0 at the
+     * same time and miss a shadow entry.
+     */
 		smp_wmb();
 	}
 	mapping->nrpages -= nr;
@@ -188,10 +188,10 @@ static void unaccount_page_cache_page(struct address_space *mapping,
 	int nr;
 
 	/*
-	 * if we're uptodate, flush out into the cleancache, otherwise
-	 * invalidate any existing cleancache entries.  We can't leave
-	 * stale data around in the cleancache once our page is gone
-	 */
+   * if we're uptodate, flush out into the cleancache, otherwise
+   * invalidate any existing cleancache entries.  We can't leave
+   * stale data around in the cleancache once our page is gone
+   */
 	if (PageUptodate(page) && PageMappedToDisk(page))
 		cleancache_put_page(page);
 	else
@@ -212,11 +212,11 @@ static void unaccount_page_cache_page(struct address_space *mapping,
 		if (mapping_exiting(mapping) &&
 		    page_count(page) >= mapcount + 2) {
 			/*
-			 * All vmas have already been torn down, so it's
-			 * a good bet that actually the page is unmapped,
-			 * and we'd prefer not to leak it: if we're wrong,
-			 * some other bad page check should catch it later.
-			 */
+       * All vmas have already been torn down, so it's
+       * a good bet that actually the page is unmapped,
+       * and we'd prefer not to leak it: if we're wrong,
+       * some other bad page check should catch it later.
+       */
 			page_mapcount_reset(page);
 			page_ref_sub(page, mapcount);
 		}
@@ -238,15 +238,15 @@ static void unaccount_page_cache_page(struct address_space *mapping,
 	}
 
 	/*
-	 * At this point page must be either written or cleaned by
-	 * truncate.  Dirty page here signals a bug and loss of
-	 * unwritten data.
-	 *
-	 * This fixes dirty accounting after removing the page entirely
-	 * but leaves PageDirty set: it has no effect for truncated
-	 * page and anyway will be cleared before returning page into
-	 * buddy allocator.
-	 */
+   * At this point page must be either written or cleaned by
+   * truncate.  Dirty page here signals a bug and loss of
+   * unwritten data.
+   *
+   * This fixes dirty accounting after removing the page entirely
+   * but leaves PageDirty set: it has no effect for truncated
+   * page and anyway will be cleared before returning page into
+   * buddy allocator.
+   */
 	if (WARN_ON_ONCE(PageDirty(page)))
 		account_page_cleaned(page, mapping, inode_to_wb(mapping->host));
 }
@@ -267,7 +267,7 @@ void __delete_from_page_cache(struct page *page, void *shadow)
 }
 
 static void page_cache_free_page(struct address_space *mapping,
-				struct page *page)
+				 struct page *page)
 {
 	void (*freepage)(struct page *);
 
@@ -319,9 +319,8 @@ EXPORT_SYMBOL(delete_from_page_cache);
  *
  * The function expects the i_pages lock to be held.
  */
-static void
-page_cache_tree_delete_batch(struct address_space *mapping,
-			     struct pagevec *pvec)
+static void page_cache_tree_delete_batch(struct address_space *mapping,
+					 struct pagevec *pvec)
 {
 	struct radix_tree_iter iter;
 	void **slot;
@@ -331,19 +330,19 @@ page_cache_tree_delete_batch(struct address_space *mapping,
 	pgoff_t start;
 
 	start = pvec->pages[0]->index;
-	radix_tree_for_each_slot(slot, &mapping->i_pages, &iter, start) {
+	radix_tree_for_each_slot (slot, &mapping->i_pages, &iter, start) {
 		if (i >= pagevec_count(pvec) && !tail_pages)
 			break;
-		page = radix_tree_deref_slot_protected(slot,
-						       &mapping->i_pages.xa_lock);
+		page = radix_tree_deref_slot_protected(
+			slot, &mapping->i_pages.xa_lock);
 		if (radix_tree_exceptional_entry(page))
 			continue;
 		if (!tail_pages) {
 			/*
-			 * Some page got inserted in our range? Skip it. We
-			 * have our pages locked so they are protected from
-			 * being removed.
-			 */
+       * Some page got inserted in our range? Skip it. We
+       * have our pages locked so they are protected from
+       * being removed.
+       */
 			if (page != pvec->pages[i])
 				continue;
 			WARN_ON_ONCE(!PageLocked(page));
@@ -351,16 +350,16 @@ page_cache_tree_delete_batch(struct address_space *mapping,
 				tail_pages = HPAGE_PMD_NR - 1;
 			page->mapping = NULL;
 			/*
-			 * Leave page->index set: truncation lookup relies
-			 * upon it
-			 */
+       * Leave page->index set: truncation lookup relies
+       * upon it
+       */
 			i++;
 		} else {
 			tail_pages--;
 		}
 		radix_tree_clear_tags(&mapping->i_pages, iter.node, slot);
 		__radix_tree_replace(&mapping->i_pages, iter.node, slot, NULL,
-				workingset_lookup_update(mapping));
+				     workingset_lookup_update(mapping));
 		total_pages++;
 	}
 	mapping->nrpages -= total_pages;
@@ -428,7 +427,28 @@ static int filemap_check_and_keep_errors(struct address_space *mapping)
  * be waited upon, and not just skipped over.
  */
 int __filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
-				loff_t end, int sync_mode)
+			       loff_t end, int sync_mode)
+{
+	int ret;
+	struct writeback_control wbc = {
+		.sync_mode = sync_mode,
+		.nr_to_write = LONG_MAX,
+		.range_start = start,
+		.range_end = end,
+	};
+
+	if (!mapping_cap_writeback_dirty(mapping))
+		return 0;
+
+	wbc_attach_fdatawrite_inode(&wbc, mapping->host);
+	ret = do_writepages(mapping, &wbc);
+	wbc_detach_inode(&wbc);
+	return ret;
+}
+
+// lxj add
+int __filemap_fstreamwrite_range(struct address_space *mapping, loff_t start,
+				 loff_t end, int sync_mode, uint sid)
 {
 	int ret;
 	struct writeback_control wbc = {
@@ -436,6 +456,7 @@ int __filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
 		.nr_to_write = LONG_MAX,
 		.range_start = start,
 		.range_end = end,
+		.stream = sid,
 	};
 
 	if (!mapping_cap_writeback_dirty(mapping))
@@ -448,7 +469,7 @@ int __filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
 }
 
 static inline int __filemap_fdatawrite(struct address_space *mapping,
-	int sync_mode)
+				       int sync_mode)
 {
 	return __filemap_fdatawrite_range(mapping, 0, LLONG_MAX, sync_mode);
 }
@@ -460,12 +481,26 @@ int filemap_fdatawrite(struct address_space *mapping)
 EXPORT_SYMBOL(filemap_fdatawrite);
 
 int filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
-				loff_t end)
+			     loff_t end)
 {
 	return __filemap_fdatawrite_range(mapping, start, end, WB_SYNC_ALL);
 }
 EXPORT_SYMBOL(filemap_fdatawrite_range);
 
+// lxj add
+int filemap_fdatawrite_tag_range(struct address_space *mapping, loff_t start,
+				 loff_t end, int tag)
+{
+	return __filemap_fdatawrite_range(mapping, start, end, tag);
+}
+EXPORT_SYMBOL(filemap_fdatawrite_tag_range);
+
+int filemap_fstreamwrite_tag_range(struct address_space *mapping, loff_t start,
+				   loff_t end, int tag, uint sid)
+{
+	return __filemap_fstreamwrite_range(mapping, start, end, tag, sid);
+}
+EXPORT_SYMBOL(filemap_fstreamwrite_tag_range);
 /**
  * filemap_flush - mostly a non-blocking flush
  * @mapping:	target address_space
@@ -488,8 +523,8 @@ EXPORT_SYMBOL(filemap_flush);
  * Find at least one page in the range supplied, usually used to check if
  * direct writing in this range will trigger a writeback.
  */
-bool filemap_range_has_page(struct address_space *mapping,
-			   loff_t start_byte, loff_t end_byte)
+bool filemap_range_has_page(struct address_space *mapping, loff_t start_byte,
+			    loff_t end_byte)
 {
 	pgoff_t index = start_byte >> PAGE_SHIFT;
 	pgoff_t end = end_byte >> PAGE_SHIFT;
@@ -509,7 +544,7 @@ bool filemap_range_has_page(struct address_space *mapping,
 EXPORT_SYMBOL(filemap_range_has_page);
 
 static void __filemap_fdatawait_range(struct address_space *mapping,
-				     loff_t start_byte, loff_t end_byte)
+				      loff_t start_byte, loff_t end_byte)
 {
 	pgoff_t index = start_byte >> PAGE_SHIFT;
 	pgoff_t end = end_byte >> PAGE_SHIFT;
@@ -523,8 +558,8 @@ static void __filemap_fdatawait_range(struct address_space *mapping,
 	while (index <= end) {
 		unsigned i;
 
-		nr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index,
-				end, PAGECACHE_TAG_WRITEBACK);
+		nr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,
+						    PAGECACHE_TAG_WRITEBACK);
 		if (!nr_pages)
 			break;
 
@@ -606,7 +641,7 @@ EXPORT_SYMBOL(filemap_fdatawait_keep_errors);
 static bool mapping_needs_writeback(struct address_space *mapping)
 {
 	return (!dax_mapping(mapping) && mapping->nrpages) ||
-	    (dax_mapping(mapping) && mapping->nrexceptional);
+	       (dax_mapping(mapping) && mapping->nrexceptional);
 }
 
 int filemap_write_and_wait(struct address_space *mapping)
@@ -616,11 +651,11 @@ int filemap_write_and_wait(struct address_space *mapping)
 	if (mapping_needs_writeback(mapping)) {
 		err = filemap_fdatawrite(mapping);
 		/*
-		 * Even if the above returned error, the pages may be
-		 * written partially (e.g. -ENOSPC), so we wait for it.
-		 * But the -EIO is special case, it may indicate the worst
-		 * thing (e.g. bug) happened, so we avoid waiting for it.
-		 */
+     * Even if the above returned error, the pages may be
+     * written partially (e.g. -ENOSPC), so we wait for it.
+     * But the -EIO is special case, it may indicate the worst
+     * thing (e.g. bug) happened, so we avoid waiting for it.
+     */
 		if (err != -EIO) {
 			int err2 = filemap_fdatawait(mapping);
 			if (!err)
@@ -647,8 +682,8 @@ EXPORT_SYMBOL(filemap_write_and_wait);
  * Note that @lend is inclusive (describes the last byte to be written) so
  * that this function can be used to write to the very end-of-file (end = -1).
  */
-int filemap_write_and_wait_range(struct address_space *mapping,
-				 loff_t lstart, loff_t lend)
+int filemap_write_and_wait_range(struct address_space *mapping, loff_t lstart,
+				 loff_t lend)
 {
 	int err = 0;
 
@@ -657,8 +692,8 @@ int filemap_write_and_wait_range(struct address_space *mapping,
 						 WB_SYNC_ALL);
 		/* See comment of filemap_write_and_wait() */
 		if (err != -EIO) {
-			int err2 = filemap_fdatawait_range(mapping,
-						lstart, lend);
+			int err2 =
+				filemap_fdatawait_range(mapping, lstart, lend);
 			if (!err)
 				err = err2;
 		} else {
@@ -714,16 +749,16 @@ int file_check_and_advance_wb_err(struct file *file)
 		spin_lock(&file->f_lock);
 		old = file->f_wb_err;
 		err = errseq_check_and_advance(&mapping->wb_err,
-						&file->f_wb_err);
+					       &file->f_wb_err);
 		trace_file_check_and_advance_wb_err(file, old);
 		spin_unlock(&file->f_lock);
 	}
 
 	/*
-	 * We're mostly using this function as a drop in replacement for
-	 * filemap_check_errors. Clear AS_EIO/AS_ENOSPC to emulate the effect
-	 * that the legacy code would have had on these flags.
-	 */
+   * We're mostly using this function as a drop in replacement for
+   * filemap_check_errors. Clear AS_EIO/AS_ENOSPC to emulate the effect
+   * that the legacy code would have had on these flags.
+   */
 	clear_bit(AS_EIO, &mapping->flags);
 	clear_bit(AS_ENOSPC, &mapping->flags);
 	return err;
@@ -805,8 +840,8 @@ int replace_page_cache_page(struct page *old, struct page *new, gfp_t gfp_mask)
 		BUG_ON(error);
 
 		/*
-		 * hugetlb pages do not participate in page cache accounting.
-		 */
+     * hugetlb pages do not participate in page cache accounting.
+     */
 		if (!PageHuge(new))
 			__inc_node_page_state(new, NR_FILE_PAGES);
 		if (PageSwapBacked(new))
@@ -836,8 +871,8 @@ static int __add_to_page_cache_locked(struct page *page,
 	VM_BUG_ON_PAGE(PageSwapBacked(page), page);
 
 	if (!huge) {
-		error = mem_cgroup_try_charge(page, current->mm,
-					      gfp_mask, &memcg, false);
+		error = mem_cgroup_try_charge(page, current->mm, gfp_mask,
+					      &memcg, false);
 		if (error)
 			return error;
 	}
@@ -888,35 +923,35 @@ static int __add_to_page_cache_locked(struct page *page,
  * This function does not add the page to the LRU.  The caller must do that.
  */
 int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
-		pgoff_t offset, gfp_t gfp_mask)
+			     pgoff_t offset, gfp_t gfp_mask)
 {
-	return __add_to_page_cache_locked(page, mapping, offset,
-					  gfp_mask, NULL);
+	return __add_to_page_cache_locked(page, mapping, offset, gfp_mask,
+					  NULL);
 }
 EXPORT_SYMBOL(add_to_page_cache_locked);
 
 int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
-				pgoff_t offset, gfp_t gfp_mask)
+			  pgoff_t offset, gfp_t gfp_mask)
 {
 	void *shadow = NULL;
 	int ret;
 
 	__SetPageLocked(page);
-	ret = __add_to_page_cache_locked(page, mapping, offset,
-					 gfp_mask, &shadow);
+	ret = __add_to_page_cache_locked(page, mapping, offset, gfp_mask,
+					 &shadow);
 	if (unlikely(ret))
 		__ClearPageLocked(page);
 	else {
 		/*
-		 * The page might have been evicted from cache only
-		 * recently, in which case it should be activated like
-		 * any other repeatedly accessed page.
-		 * The exception is pages getting rewritten; evicting other
-		 * data from the working set, only to cache data that will
-		 * get overwritten with something else, is a waste of memory.
-		 */
-		if (!(gfp_mask & __GFP_WRITE) &&
-		    shadow && workingset_refault(shadow)) {
+     * The page might have been evicted from cache only
+     * recently, in which case it should be activated like
+     * any other repeatedly accessed page.
+     * The exception is pages getting rewritten; evicting other
+     * data from the working set, only to cache data that will
+     * get overwritten with something else, is a waste of memory.
+     */
+		if (!(gfp_mask & __GFP_WRITE) && shadow &&
+		    workingset_refault(shadow)) {
 			SetPageActive(page);
 			workingset_activation(page);
 		} else
@@ -960,7 +995,8 @@ EXPORT_SYMBOL(__page_cache_alloc);
  */
 #define PAGE_WAIT_TABLE_BITS 8
 #define PAGE_WAIT_TABLE_SIZE (1 << PAGE_WAIT_TABLE_BITS)
-static wait_queue_head_t page_wait_table[PAGE_WAIT_TABLE_SIZE] __cacheline_aligned;
+static wait_queue_head_t
+	page_wait_table[PAGE_WAIT_TABLE_SIZE] __cacheline_aligned;
 
 static wait_queue_head_t *page_waitqueue(struct page *page)
 {
@@ -990,14 +1026,15 @@ struct wait_page_queue {
 	wait_queue_entry_t wait;
 };
 
-static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)
+static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync,
+			      void *arg)
 {
 	struct wait_page_key *key = arg;
-	struct wait_page_queue *wait_page
-		= container_of(wait, struct wait_page_queue, wait);
+	struct wait_page_queue *wait_page =
+		container_of(wait, struct wait_page_queue, wait);
 
 	if (wait_page->page != key->page)
-	       return 0;
+		return 0;
 	key->page_match = 1;
 
 	if (wait_page->bit_nr != key->bit_nr)
@@ -1031,11 +1068,11 @@ static void wake_up_page_bit(struct page *page, int bit_nr)
 
 	while (bookmark.flags & WQ_FLAG_BOOKMARK) {
 		/*
-		 * Take a breather from holding the lock,
-		 * allow pages that finish wake up asynchronously
-		 * to acquire the lock and remove themselves
-		 * from wait queue
-		 */
+     * Take a breather from holding the lock,
+     * allow pages that finish wake up asynchronously
+     * to acquire the lock and remove themselves
+     * from wait queue
+     */
 		spin_unlock_irqrestore(&q->lock, flags);
 		cpu_relax();
 		spin_lock_irqsave(&q->lock, flags);
@@ -1043,23 +1080,23 @@ static void wake_up_page_bit(struct page *page, int bit_nr)
 	}
 
 	/*
-	 * It is possible for other pages to have collided on the waitqueue
-	 * hash, so in that case check for a page match. That prevents a long-
-	 * term waiter
-	 *
-	 * It is still possible to miss a case here, when we woke page waiters
-	 * and removed them from the waitqueue, but there are still other
-	 * page waiters.
-	 */
+   * It is possible for other pages to have collided on the waitqueue
+   * hash, so in that case check for a page match. That prevents a long-
+   * term waiter
+   *
+   * It is still possible to miss a case here, when we woke page waiters
+   * and removed them from the waitqueue, but there are still other
+   * page waiters.
+   */
 	if (!waitqueue_active(q) || !key.page_match) {
 		ClearPageWaiters(page);
 		/*
-		 * It's possible to miss clearing Waiters here, when we woke
-		 * our page waiters, but the hashed waitqueue has waiters for
-		 * other pages on it.
-		 *
-		 * That's okay, it's a rare case. The next waker will clear it.
-		 */
+     * It's possible to miss clearing Waiters here, when we woke
+     * our page waiters, but the hashed waitqueue has waiters for
+     * other pages on it.
+     *
+     * That's okay, it's a rare case. The next waker will clear it.
+     */
 	}
 	spin_unlock_irqrestore(&q->lock, flags);
 }
@@ -1072,7 +1109,8 @@ static void wake_up_page(struct page *page, int bit)
 }
 
 static inline int wait_on_page_bit_common(wait_queue_head_t *q,
-		struct page *page, int bit_nr, int state, bool lock)
+					  struct page *page, int bit_nr,
+					  int state, bool lock)
 {
 	struct wait_page_queue wait_page;
 	wait_queue_entry_t *wait = &wait_page.wait;
@@ -1117,12 +1155,12 @@ static inline int wait_on_page_bit_common(wait_queue_head_t *q,
 	finish_wait(q, wait);
 
 	/*
-	 * A signal could leave PageWaiters set. Clearing it here if
-	 * !waitqueue_active would be possible (by open-coding finish_wait),
-	 * but still fail to catch it in the case of wait hash collision. We
-	 * already can fail to clear wait hash collision cases, so don't
-	 * bother with signals either.
-	 */
+   * A signal could leave PageWaiters set. Clearing it here if
+   * !waitqueue_active would be possible (by open-coding finish_wait),
+   * but still fail to catch it in the case of wait hash collision. We
+   * already can fail to clear wait hash collision cases, so don't
+   * bother with signals either.
+   */
 
 	return ret;
 }
@@ -1174,7 +1212,8 @@ EXPORT_SYMBOL_GPL(add_page_wait_queue);
  * being cleared, but a memory barrier should be unneccssary since it is
  * in the same byte as PG_locked.
  */
-static inline bool clear_bit_unlock_is_negative_byte(long nr, volatile void *mem)
+static inline bool clear_bit_unlock_is_negative_byte(long nr,
+						     volatile void *mem)
 {
 	clear_bit_unlock(nr, mem);
 	/* smp_mb__after_atomic(); */
@@ -1215,12 +1254,12 @@ EXPORT_SYMBOL(unlock_page);
 void end_page_writeback(struct page *page)
 {
 	/*
-	 * TestClearPageReclaim could be used here but it is an atomic
-	 * operation and overkill in this particular case. Failing to
-	 * shuffle a page marked for immediate reclaim is too mild to
-	 * justify taking an atomic operation penalty at the end of
-	 * ever page writeback.
-	 */
+   * TestClearPageReclaim could be used here but it is an atomic
+   * operation and overkill in this particular case. Failing to
+   * shuffle a page marked for immediate reclaim is too mild to
+   * justify taking an atomic operation penalty at the end of
+   * ever page writeback.
+   */
 	if (PageReclaim(page)) {
 		ClearPageReclaim(page);
 		rotate_reclaimable_page(page);
@@ -1298,9 +1337,9 @@ int __lock_page_or_retry(struct page *page, struct mm_struct *mm,
 {
 	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 		/*
-		 * CAUTION! In this case, mmap_sem is not released
-		 * even though return 0.
-		 */
+     * CAUTION! In this case, mmap_sem is not released
+     * even though return 0.
+     */
 		if (flags & FAULT_FLAG_RETRY_NOWAIT)
 			return 0;
 
@@ -1346,8 +1385,8 @@ int __lock_page_or_retry(struct page *page, struct mm_struct *mm,
  * index 10, page_cache_next_hole covering both indexes may return 10
  * if called under rcu_read_lock.
  */
-pgoff_t page_cache_next_hole(struct address_space *mapping,
-			     pgoff_t index, unsigned long max_scan)
+pgoff_t page_cache_next_hole(struct address_space *mapping, pgoff_t index,
+			     unsigned long max_scan)
 {
 	unsigned long i;
 
@@ -1387,8 +1426,8 @@ EXPORT_SYMBOL(page_cache_next_hole);
  * index 5, page_cache_prev_hole covering both indexes may return 5 if
  * called under rcu_read_lock.
  */
-pgoff_t page_cache_prev_hole(struct address_space *mapping,
-			     pgoff_t index, unsigned long max_scan)
+pgoff_t page_cache_prev_hole(struct address_space *mapping, pgoff_t index,
+			     unsigned long max_scan)
 {
 	unsigned long i;
 
@@ -1437,10 +1476,10 @@ struct page *find_get_entry(struct address_space *mapping, pgoff_t offset)
 			if (radix_tree_deref_retry(page))
 				goto repeat;
 			/*
-			 * A shadow entry of a recently evicted page,
-			 * or a swap entry from shmem/tmpfs.  Return
-			 * it without attempting to raise page count.
-			 */
+       * A shadow entry of a recently evicted page,
+       * or a swap entry from shmem/tmpfs.  Return
+       * it without attempting to raise page count.
+       */
 			goto out;
 		}
 
@@ -1455,10 +1494,10 @@ struct page *find_get_entry(struct address_space *mapping, pgoff_t offset)
 		}
 
 		/*
-		 * Has the page moved?
-		 * This is part of the lockless pagecache protocol. See
-		 * include/linux/pagemap.h for details.
-		 */
+     * Has the page moved?
+     * This is part of the lockless pagecache protocol. See
+     * include/linux/pagemap.h for details.
+     */
 		if (unlikely(page != *pagep)) {
 			put_page(head);
 			goto repeat;
@@ -1533,7 +1572,7 @@ EXPORT_SYMBOL(find_lock_entry);
  * If there is a page cache page, it is returned with an increased refcount.
  */
 struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
-	int fgp_flags, gfp_t gfp_mask)
+				int fgp_flags, gfp_t gfp_mask)
 {
 	struct page *page;
 
@@ -1569,7 +1608,8 @@ struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
 no_page:
 	if (!page && (fgp_flags & FGP_CREAT)) {
 		int err;
-		if ((fgp_flags & FGP_WRITE) && mapping_cap_account_dirty(mapping))
+		if ((fgp_flags & FGP_WRITE) &&
+		    mapping_cap_account_dirty(mapping))
 			gfp_mask |= __GFP_WRITE;
 		if (fgp_flags & FGP_NOFS)
 			gfp_mask &= ~__GFP_FS;
@@ -1621,9 +1661,9 @@ EXPORT_SYMBOL(pagecache_get_page);
  * find_get_entries() returns the number of pages and shadow entries
  * which were found.
  */
-unsigned find_get_entries(struct address_space *mapping,
-			  pgoff_t start, unsigned int nr_entries,
-			  struct page **entries, pgoff_t *indices)
+unsigned find_get_entries(struct address_space *mapping, pgoff_t start,
+			  unsigned int nr_entries, struct page **entries,
+			  pgoff_t *indices)
 {
 	void **slot;
 	unsigned int ret = 0;
@@ -1633,9 +1673,9 @@ unsigned find_get_entries(struct address_space *mapping,
 		return 0;
 
 	rcu_read_lock();
-	radix_tree_for_each_slot(slot, &mapping->i_pages, &iter, start) {
+	radix_tree_for_each_slot (slot, &mapping->i_pages, &iter, start) {
 		struct page *head, *page;
-repeat:
+	repeat:
 		page = radix_tree_deref_slot(slot);
 		if (unlikely(!page))
 			continue;
@@ -1645,10 +1685,10 @@ unsigned find_get_entries(struct address_space *mapping,
 				continue;
 			}
 			/*
-			 * A shadow entry of a recently evicted page, a swap
-			 * entry from shmem/tmpfs or a DAX entry.  Return it
-			 * without attempting to raise page count.
-			 */
+       * A shadow entry of a recently evicted page, a swap
+       * entry from shmem/tmpfs or a DAX entry.  Return it
+       * without attempting to raise page count.
+       */
 			goto export;
 		}
 
@@ -1667,8 +1707,7 @@ unsigned find_get_entries(struct address_space *mapping,
 			put_page(head);
 			goto repeat;
 		}
-export:
-		indices[ret] = iter.index;
+		export : indices[ret] = iter.index;
 		entries[ret] = page;
 		if (++ret == nr_entries)
 			break;
@@ -1710,12 +1749,12 @@ unsigned find_get_pages_range(struct address_space *mapping, pgoff_t *start,
 		return 0;
 
 	rcu_read_lock();
-	radix_tree_for_each_slot(slot, &mapping->i_pages, &iter, *start) {
+	radix_tree_for_each_slot (slot, &mapping->i_pages, &iter, *start) {
 		struct page *head, *page;
 
 		if (iter.index > end)
 			break;
-repeat:
+	repeat:
 		page = radix_tree_deref_slot(slot);
 		if (unlikely(!page))
 			continue;
@@ -1726,10 +1765,10 @@ unsigned find_get_pages_range(struct address_space *mapping, pgoff_t *start,
 				continue;
 			}
 			/*
-			 * A shadow entry of a recently evicted page,
-			 * or a swap entry from shmem/tmpfs.  Skip
-			 * over it.
-			 */
+       * A shadow entry of a recently evicted page,
+       * or a swap entry from shmem/tmpfs.  Skip
+       * over it.
+       */
 			continue;
 		}
 
@@ -1757,11 +1796,11 @@ unsigned find_get_pages_range(struct address_space *mapping, pgoff_t *start,
 	}
 
 	/*
-	 * We come here when there is no page beyond @end. We take care to not
-	 * overflow the index @start as it confuses some of the callers. This
-	 * breaks the iteration when there is page at index -1 but that is
-	 * already broken anyway.
-	 */
+   * We come here when there is no page beyond @end. We take care to not
+   * overflow the index @start as it confuses some of the callers. This
+   * breaks the iteration when there is page at index -1 but that is
+   * already broken anyway.
+   */
 	if (end == (pgoff_t)-1)
 		*start = (pgoff_t)-1;
 	else
@@ -1795,9 +1834,9 @@ unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t index,
 		return 0;
 
 	rcu_read_lock();
-	radix_tree_for_each_contig(slot, &mapping->i_pages, &iter, index) {
+	radix_tree_for_each_contig (slot, &mapping->i_pages, &iter, index) {
 		struct page *head, *page;
-repeat:
+	repeat:
 		page = radix_tree_deref_slot(slot);
 		/* The hole, there no reason to continue */
 		if (unlikely(!page))
@@ -1809,10 +1848,10 @@ unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t index,
 				continue;
 			}
 			/*
-			 * A shadow entry of a recently evicted page,
-			 * or a swap entry from shmem/tmpfs.  Stop
-			 * looking for contiguous pages.
-			 */
+       * A shadow entry of a recently evicted page,
+       * or a swap entry from shmem/tmpfs.  Stop
+       * looking for contiguous pages.
+       */
 			break;
 		}
 
@@ -1833,11 +1872,12 @@ unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t index,
 		}
 
 		/*
-		 * must check mapping and index after taking the ref.
-		 * otherwise we can get both false positives and false
-		 * negatives, which is just confusing to the caller.
-		 */
-		if (page->mapping == NULL || page_to_pgoff(page) != iter.index) {
+     * must check mapping and index after taking the ref.
+     * otherwise we can get both false positives and false
+     * negatives, which is just confusing to the caller.
+     */
+		if (page->mapping == NULL ||
+		    page_to_pgoff(page) != iter.index) {
 			put_page(page);
 			break;
 		}
@@ -1864,8 +1904,8 @@ EXPORT_SYMBOL(find_get_pages_contig);
  * @tag.   We update @index to index the next page for the traversal.
  */
 unsigned find_get_pages_range_tag(struct address_space *mapping, pgoff_t *index,
-			pgoff_t end, int tag, unsigned int nr_pages,
-			struct page **pages)
+				  pgoff_t end, int tag, unsigned int nr_pages,
+				  struct page **pages)
 {
 	struct radix_tree_iter iter;
 	void **slot;
@@ -1875,12 +1915,13 @@ unsigned find_get_pages_range_tag(struct address_space *mapping, pgoff_t *index,
 		return 0;
 
 	rcu_read_lock();
-	radix_tree_for_each_tagged(slot, &mapping->i_pages, &iter, *index, tag) {
+	radix_tree_for_each_tagged (slot, &mapping->i_pages, &iter, *index,
+				    tag) {
 		struct page *head, *page;
 
 		if (iter.index > end)
 			break;
-repeat:
+	repeat:
 		page = radix_tree_deref_slot(slot);
 		if (unlikely(!page))
 			continue;
@@ -1891,16 +1932,16 @@ unsigned find_get_pages_range_tag(struct address_space *mapping, pgoff_t *index,
 				continue;
 			}
 			/*
-			 * A shadow entry of a recently evicted page.
-			 *
-			 * Those entries should never be tagged, but
-			 * this tree walk is lockless and the tags are
-			 * looked up in bulk, one radix tree node at a
-			 * time, so there is a sizable window for page
-			 * reclaim to evict a page we saw tagged.
-			 *
-			 * Skip over it.
-			 */
+       * A shadow entry of a recently evicted page.
+       *
+       * Those entries should never be tagged, but
+       * this tree walk is lockless and the tags are
+       * looked up in bulk, one radix tree node at a
+       * time, so there is a sizable window for page
+       * reclaim to evict a page we saw tagged.
+       *
+       * Skip over it.
+       */
 			continue;
 		}
 
@@ -1928,11 +1969,11 @@ unsigned find_get_pages_range_tag(struct address_space *mapping, pgoff_t *index,
 	}
 
 	/*
-	 * We come here when we got at @end. We take care to not overflow the
-	 * index @index as it confuses some of the callers. This breaks the
-	 * iteration when there is page at index -1 but that is already broken
-	 * anyway.
-	 */
+   * We come here when we got at @end. We take care to not overflow the
+   * index @index as it confuses some of the callers. This breaks the
+   * iteration when there is page at index -1 but that is already broken
+   * anyway.
+   */
 	if (end == (pgoff_t)-1)
 		*index = (pgoff_t)-1;
 	else
@@ -1957,8 +1998,8 @@ EXPORT_SYMBOL(find_get_pages_range_tag);
  * @tag.
  */
 unsigned find_get_entries_tag(struct address_space *mapping, pgoff_t start,
-			int tag, unsigned int nr_entries,
-			struct page **entries, pgoff_t *indices)
+			      int tag, unsigned int nr_entries,
+			      struct page **entries, pgoff_t *indices)
 {
 	void **slot;
 	unsigned int ret = 0;
@@ -1968,9 +2009,10 @@ unsigned find_get_entries_tag(struct address_space *mapping, pgoff_t start,
 		return 0;
 
 	rcu_read_lock();
-	radix_tree_for_each_tagged(slot, &mapping->i_pages, &iter, start, tag) {
+	radix_tree_for_each_tagged (slot, &mapping->i_pages, &iter, start,
+				    tag) {
 		struct page *head, *page;
-repeat:
+	repeat:
 		page = radix_tree_deref_slot(slot);
 		if (unlikely(!page))
 			continue;
@@ -1981,10 +2023,10 @@ unsigned find_get_entries_tag(struct address_space *mapping, pgoff_t start,
 			}
 
 			/*
-			 * A shadow entry of a recently evicted page, a swap
-			 * entry from shmem/tmpfs or a DAX entry.  Return it
-			 * without attempting to raise page count.
-			 */
+       * A shadow entry of a recently evicted page, a swap
+       * entry from shmem/tmpfs or a DAX entry.  Return it
+       * without attempting to raise page count.
+       */
 			goto export;
 		}
 
@@ -2003,8 +2045,7 @@ unsigned find_get_entries_tag(struct address_space *mapping, pgoff_t start,
 			put_page(head);
 			goto repeat;
 		}
-export:
-		indices[ret] = iter.index;
+		export : indices[ret] = iter.index;
 		entries[ret] = page;
 		if (++ret == nr_entries)
 			break;
@@ -2030,7 +2071,7 @@ EXPORT_SYMBOL(find_get_entries_tag);
  * It is going insane. Fix it by quickly scaling down the readahead size.
  */
 static void shrink_readahead_size_eio(struct file *filp,
-					struct file_ra_state *ra)
+				      struct file_ra_state *ra)
 {
 	ra->ra_pages /= 4;
 }
@@ -2048,7 +2089,8 @@ static void shrink_readahead_size_eio(struct file *filp,
  * of the logic when it comes to error handling etc.
  */
 static ssize_t generic_file_buffered_read(struct kiocb *iocb,
-		struct iov_iter *iter, ssize_t written)
+					  struct iov_iter *iter,
+					  ssize_t written)
 {
 	struct file *filp = iocb->ki_filp;
 	struct address_space *mapping = filp->f_mapping;
@@ -2058,7 +2100,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 	pgoff_t index;
 	pgoff_t last_index;
 	pgoff_t prev_index;
-	unsigned long offset;      /* offset into pagecache page */
+	unsigned long offset; /* offset into pagecache page */
 	unsigned int prev_offset;
 	int error = 0;
 
@@ -2068,8 +2110,8 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 
 	index = *ppos >> PAGE_SHIFT;
 	prev_index = ra->prev_pos >> PAGE_SHIFT;
-	prev_offset = ra->prev_pos & (PAGE_SIZE-1);
-	last_index = (*ppos + iter->count + PAGE_SIZE-1) >> PAGE_SHIFT;
+	prev_offset = ra->prev_pos & (PAGE_SIZE - 1);
+	last_index = (*ppos + iter->count + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	offset = *ppos & ~PAGE_MASK;
 
 	for (;;) {
@@ -2079,7 +2121,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		unsigned long nr, ret;
 
 		cond_resched();
-find_page:
+	find_page:
 		if (fatal_signal_pending(current)) {
 			error = -EINTR;
 			goto out;
@@ -2089,17 +2131,15 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		if (!page) {
 			if (iocb->ki_flags & IOCB_NOWAIT)
 				goto would_block;
-			page_cache_sync_readahead(mapping,
-					ra, filp,
-					index, last_index - index);
+			page_cache_sync_readahead(mapping, ra, filp, index,
+						  last_index - index);
 			page = find_get_page(mapping, index);
 			if (unlikely(page == NULL))
 				goto no_cached_page;
 		}
 		if (PageReadahead(page)) {
-			page_cache_async_readahead(mapping,
-					ra, filp, page,
-					index, last_index - index);
+			page_cache_async_readahead(mapping, ra, filp, page,
+						   index, last_index - index);
 		}
 		if (!PageUptodate(page)) {
 			if (iocb->ki_flags & IOCB_NOWAIT) {
@@ -2108,10 +2148,10 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 			}
 
 			/*
-			 * See comment in do_read_cache_page on why
-			 * wait_on_page_locked is used to avoid unnecessarily
-			 * serialisations and why it's safe.
-			 */
+       * See comment in do_read_cache_page on why
+       * wait_on_page_locked is used to avoid unnecessarily
+       * serialisations and why it's safe.
+       */
 			error = wait_on_page_locked_killable(page);
 			if (unlikely(error))
 				goto readpage_error;
@@ -2119,7 +2159,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 				goto page_ok;
 
 			if (inode->i_blkbits == PAGE_SHIFT ||
-					!mapping->a_ops->is_partially_uptodate)
+			    !mapping->a_ops->is_partially_uptodate)
 				goto page_not_up_to_date;
 			/* pipes can't handle partially uptodate pages */
 			if (unlikely(iter->type & ITER_PIPE))
@@ -2129,20 +2169,20 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 			/* Did it get truncated before we got the lock? */
 			if (!page->mapping)
 				goto page_not_up_to_date_locked;
-			if (!mapping->a_ops->is_partially_uptodate(page,
-							offset, iter->count))
+			if (!mapping->a_ops->is_partially_uptodate(page, offset,
+								   iter->count))
 				goto page_not_up_to_date_locked;
 			unlock_page(page);
 		}
-page_ok:
+	page_ok:
 		/*
-		 * i_size must be checked after we know the page is Uptodate.
-		 *
-		 * Checking i_size after the check allows us to calculate
-		 * the correct value for "nr", which means the zero-filled
-		 * part of the page is not copied back to userspace (unless
-		 * another truncate extends the file - this is desired though).
-		 */
+     * i_size must be checked after we know the page is Uptodate.
+     *
+     * Checking i_size after the check allows us to calculate
+     * the correct value for "nr", which means the zero-filled
+     * part of the page is not copied back to userspace (unless
+     * another truncate extends the file - this is desired though).
+     */
 
 		isize = i_size_read(inode);
 		end_index = (isize - 1) >> PAGE_SHIFT;
@@ -2163,24 +2203,24 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		nr = nr - offset;
 
 		/* If users can be writing to this page using arbitrary
-		 * virtual addresses, take care about potential aliasing
-		 * before reading the page on the kernel side.
-		 */
+     * virtual addresses, take care about potential aliasing
+     * before reading the page on the kernel side.
+     */
 		if (mapping_writably_mapped(mapping))
 			flush_dcache_page(page);
 
 		/*
-		 * When a sequential read accesses a page several times,
-		 * only mark it as accessed the first time.
-		 */
+     * When a sequential read accesses a page several times,
+     * only mark it as accessed the first time.
+     */
 		if (prev_index != index || offset != prev_offset)
 			mark_page_accessed(page);
 		prev_index = index;
 
 		/*
-		 * Ok, we have the page, and it's up-to-date, so
-		 * now we can copy it to user space...
-		 */
+     * Ok, we have the page, and it's up-to-date, so
+     * now we can copy it to user space...
+     */
 
 		ret = copy_page_to_iter(page, offset, nr, iter);
 		offset += ret;
@@ -2198,13 +2238,13 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		}
 		continue;
 
-page_not_up_to_date:
+	page_not_up_to_date:
 		/* Get exclusive access to the page ... */
 		error = lock_page_killable(page);
 		if (unlikely(error))
 			goto readpage_error;
 
-page_not_up_to_date_locked:
+	page_not_up_to_date_locked:
 		/* Did it get truncated before we got the lock? */
 		if (!page->mapping) {
 			unlock_page(page);
@@ -2218,12 +2258,12 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 			goto page_ok;
 		}
 
-readpage:
+	readpage:
 		/*
-		 * A previous I/O error may have been due to temporary
-		 * failures, eg. multipath errors.
-		 * PG_error will be set again if readpage fails.
-		 */
+     * A previous I/O error may have been due to temporary
+     * failures, eg. multipath errors.
+     * PG_error will be set again if readpage fails.
+     */
 		ClearPageError(page);
 		/* Start the actual read. The read will unlock the page. */
 		error = mapping->a_ops->readpage(filp, page);
@@ -2244,8 +2284,8 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 			if (!PageUptodate(page)) {
 				if (page->mapping == NULL) {
 					/*
-					 * invalidate_mapping_pages got it
-					 */
+           * invalidate_mapping_pages got it
+           */
 					unlock_page(page);
 					put_page(page);
 					goto find_page;
@@ -2260,23 +2300,24 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 
 		goto page_ok;
 
-readpage_error:
+	readpage_error:
 		/* UHHUH! A synchronous read error occurred. Report it */
 		put_page(page);
 		goto out;
 
-no_cached_page:
+	no_cached_page:
 		/*
-		 * Ok, it wasn't cached, so we need to create a new
-		 * page..
-		 */
+     * Ok, it wasn't cached, so we need to create a new
+     * page..
+     */
 		page = page_cache_alloc(mapping);
 		if (!page) {
 			error = -ENOMEM;
 			goto out;
 		}
-		error = add_to_page_cache_lru(page, mapping, index,
-				mapping_gfp_constraint(mapping, GFP_KERNEL));
+		error = add_to_page_cache_lru(
+			page, mapping, index,
+			mapping_gfp_constraint(mapping, GFP_KERNEL));
 		if (error) {
 			put_page(page);
 			if (error == -EEXIST) {
@@ -2308,8 +2349,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
  * This is the "read_iter()" routine for all filesystems
  * that can use the page cache directly.
  */
-ssize_t
-generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
+ssize_t generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
 {
 	size_t count = iov_iter_count(iter);
 	ssize_t retval = 0;
@@ -2329,9 +2369,9 @@ generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
 						   iocb->ki_pos + count - 1))
 				return -EAGAIN;
 		} else {
-			retval = filemap_write_and_wait_range(mapping,
-						iocb->ki_pos,
-					        iocb->ki_pos + count - 1);
+			retval = filemap_write_and_wait_range(
+				mapping, iocb->ki_pos,
+				iocb->ki_pos + count - 1);
 			if (retval < 0)
 				goto out;
 		}
@@ -2346,14 +2386,14 @@ generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
 		iov_iter_revert(iter, count - iov_iter_count(iter));
 
 		/*
-		 * Btrfs can have a short DIO read if we encounter
-		 * compressed extents, so if there was an error, or if
-		 * we've already read everything we wanted to, or if
-		 * there was a short read because we hit EOF, go ahead
-		 * and return.  Otherwise fallthrough to buffered io for
-		 * the rest of the read.  Buffered reads will not work for
-		 * DAX files, so don't bother trying.
-		 */
+     * Btrfs can have a short DIO read if we encounter
+     * compressed extents, so if there was an error, or if
+     * we've already read everything we wanted to, or if
+     * there was a short read because we hit EOF, go ahead
+     * and return.  Otherwise fallthrough to buffered io for
+     * the rest of the read.  Buffered reads will not work for
+     * DAX files, so don't bother trying.
+     */
 		if (retval < 0 || !count || iocb->ki_pos >= size ||
 		    IS_DAX(inode))
 			goto out;
@@ -2399,15 +2439,14 @@ static int page_cache_read(struct file *file, pgoff_t offset, gfp_t gfp_mask)
 	return ret;
 }
 
-#define MMAP_LOTSAMISS  (100)
+#define MMAP_LOTSAMISS (100)
 
 /*
  * Synchronous readahead happens when we don't even find
  * a page in the page cache at all.
  */
 static void do_sync_mmap_readahead(struct vm_area_struct *vma,
-				   struct file_ra_state *ra,
-				   struct file *file,
+				   struct file_ra_state *ra, struct file *file,
 				   pgoff_t offset)
 {
 	struct address_space *mapping = file->f_mapping;
@@ -2429,15 +2468,15 @@ static void do_sync_mmap_readahead(struct vm_area_struct *vma,
 		ra->mmap_miss++;
 
 	/*
-	 * Do we miss much more than hit in this file? If so,
-	 * stop bothering with read-ahead. It will only hurt.
-	 */
+   * Do we miss much more than hit in this file? If so,
+   * stop bothering with read-ahead. It will only hurt.
+   */
 	if (ra->mmap_miss > MMAP_LOTSAMISS)
 		return;
 
 	/*
-	 * mmap read-around
-	 */
+   * mmap read-around
+   */
 	ra->start = max_t(long, 0, offset - ra->ra_pages / 2);
 	ra->size = ra->ra_pages;
 	ra->async_size = ra->ra_pages / 4;
@@ -2449,10 +2488,8 @@ static void do_sync_mmap_readahead(struct vm_area_struct *vma,
  * so we want to possibly extend the readahead further..
  */
 static void do_async_mmap_readahead(struct vm_area_struct *vma,
-				    struct file_ra_state *ra,
-				    struct file *file,
-				    struct page *page,
-				    pgoff_t offset)
+				    struct file_ra_state *ra, struct file *file,
+				    struct page *page, pgoff_t offset)
 {
 	struct address_space *mapping = file->f_mapping;
 
@@ -2462,8 +2499,8 @@ static void do_async_mmap_readahead(struct vm_area_struct *vma,
 	if (ra->mmap_miss > 0)
 		ra->mmap_miss--;
 	if (PageReadahead(page))
-		page_cache_async_readahead(mapping, ra, file,
-					   page, offset, ra->ra_pages);
+		page_cache_async_readahead(mapping, ra, file, page, offset,
+					   ra->ra_pages);
 }
 
 /**
@@ -2506,14 +2543,14 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 		return VM_FAULT_SIGBUS;
 
 	/*
-	 * Do we have something in the page cache already?
-	 */
+   * Do we have something in the page cache already?
+   */
 	page = find_get_page(mapping, offset);
 	if (likely(page) && !(vmf->flags & FAULT_FLAG_TRIED)) {
 		/*
-		 * We found the page, so try async readahead before
-		 * waiting for the lock.
-		 */
+     * We found the page, so try async readahead before
+     * waiting for the lock.
+     */
 		do_async_mmap_readahead(vmf->vma, ra, file, page, offset);
 	} else if (!page) {
 		/* No page in the page cache at all */
@@ -2521,7 +2558,7 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 		count_vm_event(PGMAJFAULT);
 		count_memcg_event_mm(vmf->vma->vm_mm, PGMAJFAULT);
 		ret = VM_FAULT_MAJOR;
-retry_find:
+	retry_find:
 		page = find_get_page(mapping, offset);
 		if (!page)
 			goto no_cached_page;
@@ -2541,16 +2578,16 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 	VM_BUG_ON_PAGE(page->index != offset, page);
 
 	/*
-	 * We have a locked page in the page cache, now we need to check
-	 * that it's up-to-date. If not, it is going to be due to an error.
-	 */
+   * We have a locked page in the page cache, now we need to check
+   * that it's up-to-date. If not, it is going to be due to an error.
+   */
 	if (unlikely(!PageUptodate(page)))
 		goto page_not_uptodate;
 
 	/*
-	 * Found the page and have a reference on it.
-	 * We must recheck i_size under page lock.
-	 */
+   * Found the page and have a reference on it.
+   * We must recheck i_size under page lock.
+   */
 	max_off = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
 	if (unlikely(offset >= max_off)) {
 		unlock_page(page);
@@ -2563,35 +2600,35 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 
 no_cached_page:
 	/*
-	 * We're only likely to ever get here if MADV_RANDOM is in
-	 * effect.
-	 */
+   * We're only likely to ever get here if MADV_RANDOM is in
+   * effect.
+   */
 	error = page_cache_read(file, offset, vmf->gfp_mask);
 
 	/*
-	 * The page we want has now been added to the page cache.
-	 * In the unlikely event that someone removed it in the
-	 * meantime, we'll just come back here and read it again.
-	 */
+   * The page we want has now been added to the page cache.
+   * In the unlikely event that someone removed it in the
+   * meantime, we'll just come back here and read it again.
+   */
 	if (error >= 0)
 		goto retry_find;
 
 	/*
-	 * An error return from page_cache_read can result if the
-	 * system is low on memory, or a problem occurs while trying
-	 * to schedule I/O.
-	 */
+   * An error return from page_cache_read can result if the
+   * system is low on memory, or a problem occurs while trying
+   * to schedule I/O.
+   */
 	if (error == -ENOMEM)
 		return VM_FAULT_OOM;
 	return VM_FAULT_SIGBUS;
 
 page_not_uptodate:
 	/*
-	 * Umm, take care of errors if the page isn't up-to-date.
-	 * Try to re-read it _once_. We do this synchronously,
-	 * because there really aren't any performance issues here
-	 * and we need to check for errors.
-	 */
+   * Umm, take care of errors if the page isn't up-to-date.
+   * Try to re-read it _once_. We do this synchronously,
+   * because there really aren't any performance issues here
+   * and we need to check for errors.
+   */
 	ClearPageError(page);
 	error = mapping->a_ops->readpage(file, page);
 	if (!error) {
@@ -2610,8 +2647,8 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 }
 EXPORT_SYMBOL(filemap_fault);
 
-void filemap_map_pages(struct vm_fault *vmf,
-		pgoff_t start_pgoff, pgoff_t end_pgoff)
+void filemap_map_pages(struct vm_fault *vmf, pgoff_t start_pgoff,
+		       pgoff_t end_pgoff)
 {
 	struct radix_tree_iter iter;
 	void **slot;
@@ -2622,10 +2659,10 @@ void filemap_map_pages(struct vm_fault *vmf,
 	struct page *head, *page;
 
 	rcu_read_lock();
-	radix_tree_for_each_slot(slot, &mapping->i_pages, &iter, start_pgoff) {
+	radix_tree_for_each_slot (slot, &mapping->i_pages, &iter, start_pgoff) {
 		if (iter.index > end_pgoff)
 			break;
-repeat:
+	repeat:
 		page = radix_tree_deref_slot(slot);
 		if (unlikely(!page))
 			goto next;
@@ -2653,9 +2690,8 @@ void filemap_map_pages(struct vm_fault *vmf,
 			goto repeat;
 		}
 
-		if (!PageUptodate(page) ||
-				PageReadahead(page) ||
-				PageHWPoison(page))
+		if (!PageUptodate(page) || PageReadahead(page) ||
+		    PageHWPoison(page))
 			goto skip;
 		if (!trylock_page(page))
 			goto skip;
@@ -2678,11 +2714,11 @@ void filemap_map_pages(struct vm_fault *vmf,
 			goto unlock;
 		unlock_page(page);
 		goto next;
-unlock:
+	unlock:
 		unlock_page(page);
-skip:
+	skip:
 		put_page(page);
-next:
+	next:
 		/* Huge page is mapped? No need to proceed. */
 		if (pmd_trans_huge(*vmf->pmd))
 			break;
@@ -2700,7 +2736,7 @@ vm_fault_t filemap_page_mkwrite(struct vm_fault *vmf)
 	vm_fault_t ret = VM_FAULT_LOCKED;
 
 	sb_start_pagefault(inode->i_sb);
-	file_update_time(vmf->vma->vm_file);
+	vma_file_update_time(vmf->vma);
 	lock_page(page);
 	if (page->mapping != inode->i_mapping) {
 		unlock_page(page);
@@ -2708,10 +2744,10 @@ vm_fault_t filemap_page_mkwrite(struct vm_fault *vmf)
 		goto out;
 	}
 	/*
-	 * We mark the page dirty already here so that when freeze is in
-	 * progress, we are guaranteed that writeback during freezing will
-	 * see the dirty page and writeprotect it again.
-	 */
+   * We mark the page dirty already here so that when freeze is in
+   * progress, we are guaranteed that writeback during freezing will
+   * see the dirty page and writeprotect it again.
+   */
 	set_page_dirty(page);
 	wait_for_stable_page(page);
 out:
@@ -2720,14 +2756,14 @@ vm_fault_t filemap_page_mkwrite(struct vm_fault *vmf)
 }
 
 const struct vm_operations_struct generic_file_vm_ops = {
-	.fault		= filemap_fault,
-	.map_pages	= filemap_map_pages,
-	.page_mkwrite	= filemap_page_mkwrite,
+	.fault = filemap_fault,
+	.map_pages = filemap_map_pages,
+	.page_mkwrite = filemap_page_mkwrite,
 };
 
 /* This is used for a general mmap of a disk file */
 
-int generic_file_mmap(struct file * file, struct vm_area_struct * vma)
+int generic_file_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	struct address_space *mapping = file->f_mapping;
 
@@ -2752,11 +2788,11 @@ int filemap_page_mkwrite(struct vm_fault *vmf)
 {
 	return -ENOSYS;
 }
-int generic_file_mmap(struct file * file, struct vm_area_struct * vma)
+int generic_file_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	return -ENOSYS;
 }
-int generic_file_readonly_mmap(struct file * file, struct vm_area_struct * vma)
+int generic_file_readonly_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	return -ENOSYS;
 }
@@ -2779,10 +2815,9 @@ static struct page *wait_on_page_read(struct page *page)
 }
 
 static struct page *do_read_cache_page(struct address_space *mapping,
-				pgoff_t index,
-				int (*filler)(void *, struct page *),
-				void *data,
-				gfp_t gfp)
+				       pgoff_t index,
+				       int (*filler)(void *, struct page *),
+				       void *data, gfp_t gfp)
 {
 	struct page *page;
 	int err;
@@ -2801,7 +2836,7 @@ static struct page *do_read_cache_page(struct address_space *mapping,
 			return ERR_PTR(err);
 		}
 
-filler:
+	filler:
 		err = filler(data, page);
 		if (err < 0) {
 			put_page(page);
@@ -2817,36 +2852,36 @@ static struct page *do_read_cache_page(struct address_space *mapping,
 		goto out;
 
 	/*
-	 * Page is not up to date and may be locked due one of the following
-	 * case a: Page is being filled and the page lock is held
-	 * case b: Read/write error clearing the page uptodate status
-	 * case c: Truncation in progress (page locked)
-	 * case d: Reclaim in progress
-	 *
-	 * Case a, the page will be up to date when the page is unlocked.
-	 *    There is no need to serialise on the page lock here as the page
-	 *    is pinned so the lock gives no additional protection. Even if the
-	 *    the page is truncated, the data is still valid if PageUptodate as
-	 *    it's a race vs truncate race.
-	 * Case b, the page will not be up to date
-	 * Case c, the page may be truncated but in itself, the data may still
-	 *    be valid after IO completes as it's a read vs truncate race. The
-	 *    operation must restart if the page is not uptodate on unlock but
-	 *    otherwise serialising on page lock to stabilise the mapping gives
-	 *    no additional guarantees to the caller as the page lock is
-	 *    released before return.
-	 * Case d, similar to truncation. If reclaim holds the page lock, it
-	 *    will be a race with remove_mapping that determines if the mapping
-	 *    is valid on unlock but otherwise the data is valid and there is
-	 *    no need to serialise with page lock.
-	 *
-	 * As the page lock gives no additional guarantee, we optimistically
-	 * wait on the page to be unlocked and check if it's up to date and
-	 * use the page if it is. Otherwise, the page lock is required to
-	 * distinguish between the different cases. The motivation is that we
-	 * avoid spurious serialisations and wakeups when multiple processes
-	 * wait on the same page for IO to complete.
-	 */
+   * Page is not up to date and may be locked due one of the following
+   * case a: Page is being filled and the page lock is held
+   * case b: Read/write error clearing the page uptodate status
+   * case c: Truncation in progress (page locked)
+   * case d: Reclaim in progress
+   *
+   * Case a, the page will be up to date when the page is unlocked.
+   *    There is no need to serialise on the page lock here as the page
+   *    is pinned so the lock gives no additional protection. Even if the
+   *    the page is truncated, the data is still valid if PageUptodate as
+   *    it's a race vs truncate race.
+   * Case b, the page will not be up to date
+   * Case c, the page may be truncated but in itself, the data may still
+   *    be valid after IO completes as it's a read vs truncate race. The
+   *    operation must restart if the page is not uptodate on unlock but
+   *    otherwise serialising on page lock to stabilise the mapping gives
+   *    no additional guarantees to the caller as the page lock is
+   *    released before return.
+   * Case d, similar to truncation. If reclaim holds the page lock, it
+   *    will be a race with remove_mapping that determines if the mapping
+   *    is valid on unlock but otherwise the data is valid and there is
+   *    no need to serialise with page lock.
+   *
+   * As the page lock gives no additional guarantee, we optimistically
+   * wait on the page to be unlocked and check if it's up to date and
+   * use the page if it is. Otherwise, the page lock is required to
+   * distinguish between the different cases. The motivation is that we
+   * avoid spurious serialisations and wakeups when multiple processes
+   * wait on the same page for IO to complete.
+   */
 	wait_on_page_locked(page);
 	if (PageUptodate(page))
 		goto out;
@@ -2885,12 +2920,11 @@ static struct page *do_read_cache_page(struct address_space *mapping,
  *
  * If the page does not get brought uptodate, return -EIO.
  */
-struct page *read_cache_page(struct address_space *mapping,
-				pgoff_t index,
-				int (*filler)(void *, struct page *),
-				void *data)
+struct page *read_cache_page(struct address_space *mapping, pgoff_t index,
+			     int (*filler)(void *, struct page *), void *data)
 {
-	return do_read_cache_page(mapping, index, filler, data, mapping_gfp_mask(mapping));
+	return do_read_cache_page(mapping, index, filler, data,
+				  mapping_gfp_mask(mapping));
 }
 EXPORT_SYMBOL(read_cache_page);
 
@@ -2905,9 +2939,8 @@ EXPORT_SYMBOL(read_cache_page);
  *
  * If the page does not get brought uptodate, return -EIO.
  */
-struct page *read_cache_page_gfp(struct address_space *mapping,
-				pgoff_t index,
-				gfp_t gfp)
+struct page *read_cache_page_gfp(struct address_space *mapping, pgoff_t index,
+				 gfp_t gfp)
 {
 	filler_t *filler = (filler_t *)mapping->a_ops->readpage;
 
@@ -2950,22 +2983,22 @@ inline ssize_t generic_write_checks(struct kiocb *iocb, struct iov_iter *from)
 	}
 
 	/*
-	 * LFS rule
-	 */
+   * LFS rule
+   */
 	if (unlikely(pos + iov_iter_count(from) > MAX_NON_LFS &&
-				!(file->f_flags & O_LARGEFILE))) {
+		     !(file->f_flags & O_LARGEFILE))) {
 		if (pos >= MAX_NON_LFS)
 			return -EFBIG;
 		iov_iter_truncate(from, MAX_NON_LFS - (unsigned long)pos);
 	}
 
 	/*
-	 * Are we about to exceed the fs block limit ?
-	 *
-	 * If we have written data it becomes a short write.  If we have
-	 * exceeded without writing data we send a signal and return EFBIG.
-	 * Linus frestrict idea will clean these up nicely..
-	 */
+   * Are we about to exceed the fs block limit ?
+   *
+   * If we have written data it becomes a short write.  If we have
+   * exceeded without writing data we send a signal and return EFBIG.
+   * Linus frestrict idea will clean these up nicely..
+   */
 	if (unlikely(pos >= inode->i_sb->s_maxbytes))
 		return -EFBIG;
 
@@ -2975,19 +3008,18 @@ inline ssize_t generic_write_checks(struct kiocb *iocb, struct iov_iter *from)
 EXPORT_SYMBOL(generic_write_checks);
 
 int pagecache_write_begin(struct file *file, struct address_space *mapping,
-				loff_t pos, unsigned len, unsigned flags,
-				struct page **pagep, void **fsdata)
+			  loff_t pos, unsigned len, unsigned flags,
+			  struct page **pagep, void **fsdata)
 {
 	const struct address_space_operations *aops = mapping->a_ops;
 
-	return aops->write_begin(file, mapping, pos, len, flags,
-							pagep, fsdata);
+	return aops->write_begin(file, mapping, pos, len, flags, pagep, fsdata);
 }
 EXPORT_SYMBOL(pagecache_write_begin);
 
 int pagecache_write_end(struct file *file, struct address_space *mapping,
-				loff_t pos, unsigned len, unsigned copied,
-				struct page *page, void *fsdata)
+			loff_t pos, unsigned len, unsigned copied,
+			struct page *page, void *fsdata)
 {
 	const struct address_space_operations *aops = mapping->a_ops;
 
@@ -2995,16 +3027,15 @@ int pagecache_write_end(struct file *file, struct address_space *mapping,
 }
 EXPORT_SYMBOL(pagecache_write_end);
 
-ssize_t
-generic_file_direct_write(struct kiocb *iocb, struct iov_iter *from)
+ssize_t generic_file_direct_write(struct kiocb *iocb, struct iov_iter *from)
 {
-	struct file	*file = iocb->ki_filp;
+	struct file *file = iocb->ki_filp;
 	struct address_space *mapping = file->f_mapping;
-	struct inode	*inode = mapping->host;
-	loff_t		pos = iocb->ki_pos;
-	ssize_t		written;
-	size_t		write_len;
-	pgoff_t		end;
+	struct inode *inode = mapping->host;
+	loff_t pos = iocb->ki_pos;
+	ssize_t written;
+	size_t write_len;
+	pgoff_t end;
 
 	write_len = iov_iter_count(from);
 	end = (pos + write_len - 1) >> PAGE_SHIFT;
@@ -3016,23 +3047,23 @@ generic_file_direct_write(struct kiocb *iocb, struct iov_iter *from)
 			return -EAGAIN;
 	} else {
 		written = filemap_write_and_wait_range(mapping, pos,
-							pos + write_len - 1);
+						       pos + write_len - 1);
 		if (written)
 			goto out;
 	}
 
 	/*
-	 * After a write we want buffered reads to be sure to go to disk to get
-	 * the new data.  We invalidate clean cached page from the region we're
-	 * about to write.  We do this *before* the write so that we can return
-	 * without clobbering -EIOCBQUEUED from ->direct_IO().
-	 */
-	written = invalidate_inode_pages2_range(mapping,
-					pos >> PAGE_SHIFT, end);
+   * After a write we want buffered reads to be sure to go to disk to get
+   * the new data.  We invalidate clean cached page from the region we're
+   * about to write.  We do this *before* the write so that we can return
+   * without clobbering -EIOCBQUEUED from ->direct_IO().
+   */
+	written =
+		invalidate_inode_pages2_range(mapping, pos >> PAGE_SHIFT, end);
 	/*
-	 * If a page can not be invalidated, return 0 to fall back
-	 * to buffered write.
-	 */
+   * If a page can not be invalidated, return 0 to fall back
+   * to buffered write.
+   */
 	if (written) {
 		if (written == -EBUSY)
 			return 0;
@@ -3042,21 +3073,20 @@ generic_file_direct_write(struct kiocb *iocb, struct iov_iter *from)
 	written = mapping->a_ops->direct_IO(iocb, from);
 
 	/*
-	 * Finally, try again to invalidate clean pages which might have been
-	 * cached by non-direct readahead, or faulted in by get_user_pages()
-	 * if the source of the write was an mmap'ed region of the file
-	 * we're writing.  Either one is a pretty crazy thing to do,
-	 * so we don't support it 100%.  If this invalidation
-	 * fails, tough, the write still worked...
-	 *
-	 * Most of the time we do not need this since dio_complete() will do
-	 * the invalidation for us. However there are some file systems that
-	 * do not end up with dio_complete() being called, so let's not break
-	 * them by removing it completely
-	 */
+   * Finally, try again to invalidate clean pages which might have been
+   * cached by non-direct readahead, or faulted in by get_user_pages()
+   * if the source of the write was an mmap'ed region of the file
+   * we're writing.  Either one is a pretty crazy thing to do,
+   * so we don't support it 100%.  If this invalidation
+   * fails, tough, the write still worked...
+   *
+   * Most of the time we do not need this since dio_complete() will do
+   * the invalidation for us. However there are some file systems that
+   * do not end up with dio_complete() being called, so let's not break
+   * them by removing it completely
+   */
 	if (mapping->nrpages)
-		invalidate_inode_pages2_range(mapping,
-					pos >> PAGE_SHIFT, end);
+		invalidate_inode_pages2_range(mapping, pos >> PAGE_SHIFT, end);
 
 	if (written > 0) {
 		pos += written;
@@ -3078,16 +3108,16 @@ EXPORT_SYMBOL(generic_file_direct_write);
  * page. This function is specifically for buffered writes.
  */
 struct page *grab_cache_page_write_begin(struct address_space *mapping,
-					pgoff_t index, unsigned flags)
+					 pgoff_t index, unsigned flags)
 {
 	struct page *page;
-	int fgp_flags = FGP_LOCK|FGP_WRITE|FGP_CREAT;
+	int fgp_flags = FGP_LOCK | FGP_WRITE | FGP_CREAT;
 
 	if (flags & AOP_FLAG_NOFS)
 		fgp_flags |= FGP_NOFS;
 
 	page = pagecache_get_page(mapping, index, fgp_flags,
-			mapping_gfp_mask(mapping));
+				  mapping_gfp_mask(mapping));
 	if (page)
 		wait_for_stable_page(page);
 
@@ -3095,8 +3125,7 @@ struct page *grab_cache_page_write_begin(struct address_space *mapping,
 }
 EXPORT_SYMBOL(grab_cache_page_write_begin);
 
-ssize_t generic_perform_write(struct file *file,
-				struct iov_iter *i, loff_t pos)
+ssize_t generic_perform_write(struct file *file, struct iov_iter *i, loff_t pos)
 {
 	struct address_space *mapping = file->f_mapping;
 	const struct address_space_operations *a_ops = mapping->a_ops;
@@ -3106,26 +3135,26 @@ ssize_t generic_perform_write(struct file *file,
 
 	do {
 		struct page *page;
-		unsigned long offset;	/* Offset into pagecache page */
-		unsigned long bytes;	/* Bytes to write to page */
-		size_t copied;		/* Bytes copied from user */
+		unsigned long offset; /* Offset into pagecache page */
+		unsigned long bytes; /* Bytes to write to page */
+		size_t copied; /* Bytes copied from user */
 		void *fsdata;
 
 		offset = (pos & (PAGE_SIZE - 1));
 		bytes = min_t(unsigned long, PAGE_SIZE - offset,
-						iov_iter_count(i));
+			      iov_iter_count(i));
 
-again:
+	again:
 		/*
-		 * Bring in the user page that we will copy from _first_.
-		 * Otherwise there's a nasty deadlock on copying from the
-		 * same page as we're writing to, without it being marked
-		 * up-to-date.
-		 *
-		 * Not only is this an optimisation, but it is also required
-		 * to check that the address is actually valid, when atomic
-		 * usercopies are used, below.
-		 */
+     * Bring in the user page that we will copy from _first_.
+     * Otherwise there's a nasty deadlock on copying from the
+     * same page as we're writing to, without it being marked
+     * up-to-date.
+     *
+     * Not only is this an optimisation, but it is also required
+     * to check that the address is actually valid, when atomic
+     * usercopies are used, below.
+     */
 		if (unlikely(iov_iter_fault_in_readable(i, bytes))) {
 			status = -EFAULT;
 			break;
@@ -3137,7 +3166,7 @@ ssize_t generic_perform_write(struct file *file,
 		}
 
 		status = a_ops->write_begin(file, mapping, pos, bytes, flags,
-						&page, &fsdata);
+					    &page, &fsdata);
 		if (unlikely(status < 0))
 			break;
 
@@ -3148,7 +3177,7 @@ ssize_t generic_perform_write(struct file *file,
 		flush_dcache_page(page);
 
 		status = a_ops->write_end(file, mapping, pos, bytes, copied,
-						page, fsdata);
+					  page, fsdata);
 		if (unlikely(status < 0))
 			break;
 		copied = status;
@@ -3158,15 +3187,15 @@ ssize_t generic_perform_write(struct file *file,
 		iov_iter_advance(i, copied);
 		if (unlikely(copied == 0)) {
 			/*
-			 * If we were unable to copy any data at all, we must
-			 * fall back to a single segment length write.
-			 *
-			 * If we didn't fallback here, we could livelock
-			 * because not all segments in the iov can be copied at
-			 * once without a pagefault.
-			 */
+       * If we were unable to copy any data at all, we must
+       * fall back to a single segment length write.
+       *
+       * If we didn't fallback here, we could livelock
+       * because not all segments in the iov can be copied at
+       * once without a pagefault.
+       */
 			bytes = min_t(unsigned long, PAGE_SIZE - offset,
-						iov_iter_single_seg_count(i));
+				      iov_iter_single_seg_count(i));
 			goto again;
 		}
 		pos += copied;
@@ -3199,11 +3228,11 @@ EXPORT_SYMBOL(generic_perform_write);
 ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 {
 	struct file *file = iocb->ki_filp;
-	struct address_space * mapping = file->f_mapping;
-	struct inode 	*inode = mapping->host;
-	ssize_t		written = 0;
-	ssize_t		err;
-	ssize_t		status;
+	struct address_space *mapping = file->f_mapping;
+	struct inode *inode = mapping->host;
+	ssize_t written = 0;
+	ssize_t err;
+	ssize_t status;
 
 	/* We can write back this queue in page reclaim */
 	current->backing_dev_info = inode_to_bdi(inode);
@@ -3220,45 +3249,44 @@ ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 
 		written = generic_file_direct_write(iocb, from);
 		/*
-		 * If the write stopped short of completing, fall back to
-		 * buffered writes.  Some filesystems do this for writes to
-		 * holes, for example.  For DAX files, a buffered write will
-		 * not succeed (even if it did, DAX does not handle dirty
-		 * page-cache pages correctly).
-		 */
+     * If the write stopped short of completing, fall back to
+     * buffered writes.  Some filesystems do this for writes to
+     * holes, for example.  For DAX files, a buffered write will
+     * not succeed (even if it did, DAX does not handle dirty
+     * page-cache pages correctly).
+     */
 		if (written < 0 || !iov_iter_count(from) || IS_DAX(inode))
 			goto out;
 
 		status = generic_perform_write(file, from, pos = iocb->ki_pos);
 		/*
-		 * If generic_perform_write() returned a synchronous error
-		 * then we want to return the number of bytes which were
-		 * direct-written, or the error code if that was zero.  Note
-		 * that this differs from normal direct-io semantics, which
-		 * will return -EFOO even if some bytes were written.
-		 */
+     * If generic_perform_write() returned a synchronous error
+     * then we want to return the number of bytes which were
+     * direct-written, or the error code if that was zero.  Note
+     * that this differs from normal direct-io semantics, which
+     * will return -EFOO even if some bytes were written.
+     */
 		if (unlikely(status < 0)) {
 			err = status;
 			goto out;
 		}
 		/*
-		 * We need to ensure that the page cache pages are written to
-		 * disk and invalidated to preserve the expected O_DIRECT
-		 * semantics.
-		 */
+     * We need to ensure that the page cache pages are written to
+     * disk and invalidated to preserve the expected O_DIRECT
+     * semantics.
+     */
 		endbyte = pos + status - 1;
 		err = filemap_write_and_wait_range(mapping, pos, endbyte);
 		if (err == 0) {
 			iocb->ki_pos = endbyte + 1;
 			written += status;
-			invalidate_mapping_pages(mapping,
-						 pos >> PAGE_SHIFT,
+			invalidate_mapping_pages(mapping, pos >> PAGE_SHIFT,
 						 endbyte >> PAGE_SHIFT);
 		} else {
 			/*
-			 * We don't know how much we wrote, so just return
-			 * the number of bytes which were direct-written
-			 */
+       * We don't know how much we wrote, so just return
+       * the number of bytes which were direct-written
+       */
 		}
 	} else {
 		written = generic_perform_write(file, from, iocb->ki_pos);
@@ -3317,7 +3345,7 @@ EXPORT_SYMBOL(generic_file_write_iter);
  */
 int try_to_release_page(struct page *page, gfp_t gfp_mask)
 {
-	struct address_space * const mapping = page->mapping;
+	struct address_space *const mapping = page->mapping;
 
 	BUG_ON(!PageLocked(page));
 	if (PageWriteback(page))
